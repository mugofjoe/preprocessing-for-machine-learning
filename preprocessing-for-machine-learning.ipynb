{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c23d2743-1e79-4e8c-b04a-33901618472d",
   "metadata": {},
   "source": [
    "# Preprocessing for Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c456e09d",
   "metadata": {},
   "source": [
    "Skills and best practices for preparing data for modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f36b3b",
   "metadata": {},
   "source": [
    "## What is data preprocessing?\n",
    "\n",
    "Data preprocessing is a crucial step that comes after exploring and cleaning our dataset. Once we understand the dataset's contents, structure, and quality, we typically form an idea of how we want to model it. Preprocessing involves getting the data ready for modeling, often by transforming categorical features into numerical ones since most machine learning models in Python require numerical input. This step is essential and common in data analysis and modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292e1a3c",
   "metadata": {},
   "source": [
    "## Why preprocess?\n",
    "\n",
    "The goal of preprocessing is not only to transform our dataset into a form that suitable for modeling, but also to improve the performance of our models, and in turn, produce more reliable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9933d16-e9b6-43fe-bdb6-4964923f27a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # type: ignore\n",
    "import numpy as np # type: ignore\n",
    "import matplotlib.pyplot as plt # type: ignore\n",
    "import seaborn as sns # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c433e7-eb2a-49cc-8848-48da68ddb223",
   "metadata": {},
   "source": [
    "## Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bf4c71a-9a3d-4f3b-be7d-0c45140d60a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully fetched hiking data.\n",
      "Successfully fetched wine data.\n",
      "Successfully fetched ufo data.\n",
      "Successfully fetched volunteer data.\n"
     ]
    }
   ],
   "source": [
    "# Dictionary with dataset names as keys and URLs as values\n",
    "dataset_urls = {\n",
    "    'hiking': 'https://assets.datacamp.com/production/repositories/1816/datasets/4f26c48451bdbf73db8a58e226cd3d6b45cf7bb5/hiking.json',\n",
    "    'wine': 'https://assets.datacamp.com/production/repositories/1816/datasets/9bd5350dfdb481e0f94eeef6acf2663452a8ef8b/wine_types.csv',\n",
    "    'ufo':'https://assets.datacamp.com/production/repositories/1816/datasets/a5ebfe5d2ed194f2668867603b563963af4769e9/ufo_sightings_large.csv',\n",
    "    'volunteer':'https://assets.datacamp.com/production/repositories/1816/datasets/668b96955d8b252aa8439c7602d516634e3f015e/volunteer_opportunities.csv'\n",
    "}\n",
    "\n",
    "def fetch_data_and_create_dataframe(dataset_urls):\n",
    "    dataframes = {}  # Dictionary to store DataFrames\n",
    "\n",
    "    for dataset, url in dataset_urls.items():\n",
    "        try:\n",
    "            # Determine file format based on the dataset name\n",
    "            file_format = 'json' if dataset == 'hiking' else 'csv'\n",
    "            \n",
    "            # Read the data from the URL into a DataFrame\n",
    "            if file_format == 'json':\n",
    "                df = pd.read_json(url)\n",
    "            else:\n",
    "                df = pd.read_csv(url)\n",
    "\n",
    "            dataframes[dataset] = df\n",
    "            print(f\"Successfully fetched {dataset} data.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {dataset} data: {str(e)}\")\n",
    "\n",
    "    return dataframes\n",
    "\n",
    "# Call the function with your dataset URLs\n",
    "fetched_dataframes = fetch_data_and_create_dataframe(dataset_urls)\n",
    "\n",
    "# Now 'resulting_dataframes' is a dictionary where keys are dataset names and values are DataFrames\n",
    "# You can access each DataFrame using, for example, fetched_dataframes['hiking']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de02808b",
   "metadata": {},
   "source": [
    "One of the first steps after importing data is to inspect it, which we can do with the `head()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c414ff-887b-44c3-b644-dd724a4a4b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the hiking dataset\n",
    "hiking = fetched_dataframes['hiking']\n",
    "display(hiking.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296be8e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "998ddddc-8ef1-4ba5-a471-d809d319da29",
   "metadata": {},
   "source": [
    "## Exploring with pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7acb78d-2f7f-4653-b167-5f40d7a9c8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of records with missing values\n",
    "print(hiking.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebfc14d-5131-40f6-a892-ca320e2af5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(fetched_dataframes['wine'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78e928f-d1c2-440a-a539-d2dec4003439",
   "metadata": {},
   "source": [
    "## Removing missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130cc7ab-53ee-478c-a789-c361277e57d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'A': [1.0, 4.0, 7.0, np.nan, 5.0],\n",
    "    'B': [np.nan, 7.0, np.nan, 7.0, 9.0],\n",
    "    'C': [2.0, 3.0, np.nan, np.nan, 7.0],\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced91af7-60bc-4a57-876f-45b34cdb7afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all rows containing missing values\n",
    "display(df.dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4f136f-4a1b-444a-88ae-174a5d288875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop specific rows using index labels (defaults to dropping rows)\n",
    "display(df.drop([1, 2, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e34b1a-4dd8-4e12-b849-e71eff531d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop a specific column especially if most or all of its columns are missing\n",
    "# axis=1 means we want to drop a column rather than a row\n",
    "display(df.drop(\"A\", axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4864886-911f-4640-b93c-47dedbdb9283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows where data is missing in a particular column\n",
    "\n",
    "# first - how many values we have in each column\n",
    "display(df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13363edf-878d-441c-8905-d83e3cd57cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# second - specify a list of labels to dropna\n",
    "# here, drop those rows where there's missing values in column B\n",
    "display(df.dropna(subset=[\"B\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9e4c30-2713-42d8-9432-4991ce1e861b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify how many missing values we require in each row\n",
    "display(df.dropna(thresh=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3db057-7109-4707-83cf-3cb0de930d52",
   "metadata": {},
   "source": [
    "## Merging datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272edec8-2f2f-441f-9765-a807bf552096",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Gas prices dataset\n",
    "gas_prices_data = {'date': ['2023-01-01', '2023-01-05', '2023-02-15', '2023-03-17', '2023-04-23', '2023-04-24'],\n",
    "                   'price': [2.00, 3.00, 2.00, 1.00, 3.00, 2.50]}\n",
    "gas_prices_df = pd.DataFrame(gas_prices_data)\n",
    "\n",
    "# Shipment history dataset\n",
    "shipment_history_data = {'date': ['2023-01-01', '2023-01-02', '2023-02-15', '2023-03-01', '2023-04-23', '2023-05-11'],\n",
    "                          'quantity': [1000, 5000, 500, 200, 1500, 2500]}\n",
    "shipment_history_df = pd.DataFrame(shipment_history_data)\n",
    "\n",
    "# Car sales dataset\n",
    "car_sales_data = {'date': ['2023-03-02', '2023-03-27', '2023-04-28', '2023-05-15', '2023-07-06', '2023-07-23', '2023-08-09', '2023-08-17'],\n",
    "                  'sales': [5020, 10020, 30102, 200, 1500, 2500, 500, 2150]}\n",
    "car_sales_df = pd.DataFrame(car_sales_data)\n",
    "\n",
    "# Merging all three datasets on 'date' with a full outer join\n",
    "merged_data = pd.merge(gas_prices_df, shipment_history_df, on='date', how='outer')\n",
    "merged_data = pd.merge(merged_data, car_sales_df, on='date', how='outer')\n",
    "\n",
    "# Sorting the DataFrame by 'date' to ensure correct filling order\n",
    "merged_data['date'] = pd.to_datetime(merged_data['date'])\n",
    "merged_data.sort_values(by='date', inplace=True)\n",
    "\n",
    "# Filling missing values based on the specified method\n",
    "merged_data.ffill(inplace=True)\n",
    "merged_data.bfill(inplace=True)\n",
    "\n",
    "# If you want to see the result\n",
    "print(merged_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe49ae9-b9ed-4b18-9230-92a5a9e31450",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have already merged and filled the data as mentioned in the previous code\n",
    "\n",
    "# Plotting the line graph\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.plot(merged_data['date'], merged_data['price'], label='Gas Price', marker='o')\n",
    "plt.plot(merged_data['date'], merged_data['quantity'], label='Shipment Quantity', marker='o')\n",
    "plt.plot(merged_data['date'], merged_data['sales'], label='Car Sales', marker='o')\n",
    "\n",
    "plt.title('Gas Prices, Shipment Quantity, and Car Sales Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e043d184-c0e3-4e05-b973-098811419a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have already merged and filled the data as mentioned in the previous code\n",
    "\n",
    "# Set Seaborn style\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "# Plotting the line graph with Seaborn\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "sns.lineplot(x='date', y='price', data=merged_data, label='Gas Price', marker='o')\n",
    "sns.lineplot(x='date', y='quantity', data=merged_data, label='Shipment Quantity', marker='o')\n",
    "sns.lineplot(x='date', y='sales', data=merged_data, label='Car Sales', marker='o')\n",
    "\n",
    "plt.title('Gas Prices, Shipment Quantity, and Car Sales Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca524622-333b-44db-8c26-b4e44ac4398d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have already merged and filled the data as mentioned in the previous code\n",
    "\n",
    "# Set Seaborn style\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "# Plotting the smoothened line graph with Seaborn\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "sns.lineplot(x='date', y='price', data=merged_data, label='Gas Price', marker='o', err_style=None, estimator='lowess')\n",
    "sns.lineplot(x='date', y='quantity', data=merged_data, label='Shipment Quantity', marker='o', err_style=None, estimator='lowess')\n",
    "sns.lineplot(x='date', y='sales', data=merged_data, label='Car Sales', marker='o', err_style=None, estimator='lowess')\n",
    "\n",
    "plt.title('Gas Prices, Shipment Quantity, and Car Sales Over Time (Smoothened)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af14493-85da-41d0-ad57-a7a94cdd7cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the wine dataset\n",
    "wine = fetched_dataframes['wine']\n",
    "display(wine.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15468262-938f-4c50-ba0f-c38019cb7133",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(wine.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f5b1e9-0b68-476a-935b-88953d0ca7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "volunteer = fetched_dataframes[\"volunteer\"]\n",
    "display(volunteer.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50060ec3-0c7a-4125-80f1-2185fbeba223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the Latitude and Longitude columns \n",
    "volunteer_cols = volunteer.drop([\"Latitude\",\"Longitude\"], axis=1)\n",
    "volunteer_cols.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100de5c5-4efe-4fd2-b88c-6abc6d23b7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset volunteer_cols by dropping rows containing missing values in the category_desc.\n",
    "volunteer_subset = volunteer_cols.dropna(subset=[\"category_desc\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e471bd-303a-4ee8-b1dd-26a32480d1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Verify\n",
    "volunteer_subset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4740aa-ab2b-446d-8266-975419205dc7",
   "metadata": {},
   "source": [
    "## Working with data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd12455b-885c-41be-9702-c6a99e766822",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(volunteer.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892e8595-b15b-4d5e-86b4-fccba8e806a2",
   "metadata": {},
   "source": [
    "`datetime64` data type unlocks time series functionality.\n",
    "datetime64 is another common data type that stores date and time data. This special data type unlocks a bunch of extra functionality for working with time series data, such as datetime indexing, adding timezone information, and selecting a datetime sampling frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a58927-1d33-4d0b-97e8-fde3e3ac43dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40aa766a-fec5-4be1-8a30-ecd2afc30562",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
